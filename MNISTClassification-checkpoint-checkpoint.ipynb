{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5265979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5508ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b75e359",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = ds.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde20083",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93c99f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19d1e48ac40>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_tensor = torch.Tensor(X_train).reshape(len(X_train), 1, 28, 28)\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "axs[0, 0].imshow(X_train_tensor[0].permute(1, 2, 0).detach().cpu())\n",
    "axs[0, 1].imshow(X_train_tensor[1].permute(1, 2, 0).detach().cpu())\n",
    "axs[1, 0].imshow(X_train_tensor[2].permute(1, 2, 0).detach().cpu())\n",
    "axs[1, 1].imshow(X_train_tensor[3].permute(1, 2, 0).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7018b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01bcc3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = len(X_train)\n",
    "N_test = len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8addd37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1317167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a093437",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.Tensor(X_train).reshape(N_train, 1, 28, 28)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "X_test_tensor = torch.Tensor(X_test).reshape(N_test, 1, 28, 28)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00083409",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngf = 28 #size of feature maps for dis\n",
    "ndf = 28 #size of feature maps for gen\n",
    "nc = 1 #num of channels\n",
    "nz=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "id": "9ee1de13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.label_emb = nn.Embedding(10, 10)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(110, 784), \n",
    "            nn.Unflatten(1, (16, 7, 7)),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Conv2d(32, 32, kernel_size=3, padding=1),  \n",
    "            #nn.ReLU(),\n",
    "            #nn.Conv2d(32, 16, kernel_size=3, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z, labels):\n",
    "        z = z.view(z.size(0), 100)\n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat([z, c], 1)\n",
    "        out = self.model(x)\n",
    "        return out.view(x.size(0), 28, 28)\n",
    "\n",
    "#     def forward(self, z, labels):\n",
    "#         z = z.view(z.size(0), 90)\n",
    "#         c = self.labels_emb(labels)\n",
    "#         x = torch.cat([z, c], 1)\n",
    "#         x=x.view(x.shape[0], x.shape[1], 1, 1)\n",
    "#         print(x.shape)\n",
    "#         out = self.feature(x)\n",
    "#         print(out.shape)\n",
    "#         return out.view(out.size(0), 28, 28)\n",
    "        \n",
    "\n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.label_emb = nn.Embedding(10, 10)\n",
    "        \n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(794, 1024),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(1024, 512),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(512, 256),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(256, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x, labels):\n",
    "#         x = x.view(x.size(0), 784)\n",
    "#         c = self.label_emb(labels)\n",
    "#         x = torch.cat([x, c], 1)\n",
    "#         out = self.model(x)\n",
    "#         return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "id": "b8cb77c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.labels_emb = nn.Embedding(10, 10)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(794, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, z, labels):\n",
    "        z=z.view(z.size(0), 784)\n",
    "        c = self.labels_emb(labels)\n",
    "        x = torch.cat([z, c], 1)\n",
    "        out = self.model(x)\n",
    "        return torch.reshape(out, (batch_size,))\n",
    "    \n",
    "# class generator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(generator, self).__init__()\n",
    "#         self.labels_emb = nn.Embedding(10, 10)\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(110, 784), \n",
    "#             nn.Unflatten(1, (16, 7, 7)),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(16, 16, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             #nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "#             nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             #nn.Conv2d(32, 32, kernel_size=3, padding=1),  \n",
    "#             #nn.ReLU(),\n",
    "#             #nn.Conv2d(32, 16, kernel_size=3, padding=1),  \n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(8, 1, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "        \n",
    "# #         self.fc1 = nn.Linear(128, 1024)\n",
    "# #         self.fc2 = nn.Linear(1024, 2048)\n",
    "# #         self.fc3 = nn.Linear(2048, 784)\n",
    "# #         self.activation1=nn.ReLU()\n",
    "# #         self.activation2=nn.ReLU()\n",
    "#     def forward(self, z, labels):\n",
    "# #         x=self.activation1(self.fc1(x))\n",
    "# #         x=self.activation2(self.fc2(x))\n",
    "# #         x=self.fc3(x)\n",
    "# #         x=x.view(-1, 1, 28, 28)\n",
    "#         z = z.view(z.size(0), 100)\n",
    "#         c = self.labels_emb(labels)\n",
    "#         x = torch.cat([z, c], 1)\n",
    "#         out = self.model(x)\n",
    "#         return out.view(x.size(0), 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5aaecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "        self.activation = nn.LeakyReLU(0.1)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=x.view(-1, 784)\n",
    "        x=self.activation(self.fc1(x))\n",
    "        x=self.fc2(x)\n",
    "        x=self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(128, 784), \n",
    "            nn.Unflatten(1, (16, 7, 7)),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Conv2d(32, 32, kernel_size=3, padding=1),  \n",
    "            #nn.ReLU(),\n",
    "            #nn.Conv2d(32, 16, kernel_size=3, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "#         self.fc1 = nn.Linear(128, 1024)\n",
    "#         self.fc2 = nn.Linear(1024, 2048)\n",
    "#         self.fc3 = nn.Linear(2048, 784)\n",
    "#         self.activation1=nn.ReLU()\n",
    "#         self.activation2=nn.ReLU()\n",
    "    def forward(self, x):\n",
    "#         x=self.activation1(self.fc1(x))\n",
    "#         x=self.activation2(self.fc2(x))\n",
    "#         x=self.fc3(x)\n",
    "#         x=x.view(-1, 1, 28, 28)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74479907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19d20625d60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train_tensor[0].permute(1, 2, 0).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5013928c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 14, 14])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.ConvTranspose2d(16, 16, kernel_size=4, stride=2, padding=1)(torch.randn(1, 16, 7, 7)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a66d51e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3194cdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "# lr = 1.5e-3\n",
    "loss_G = nn.BCELoss()\n",
    "loss_D = nn.BCELoss()\n",
    "\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "G_optimizer = optim.Adam(G.parameters(), betas=(0.5, 0.999))\n",
    "D_optimizer = optim.Adam(D.parameters(), betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e1eb376",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6484/2362863012.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "G(torch.rand(batch_size, 100, 1).cuda(), torch.LongTensor(np.random.randint(0, 10, batch_size)).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "id": "597842be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0157, 0.2706, 0.5804,\n",
       "          0.8235, 0.9922, 0.9922, 0.9922, 0.9922, 0.9961, 0.9922, 0.9922,\n",
       "          0.9922, 0.3020, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0863, 0.9882, 0.9882,\n",
       "          0.9922, 0.9882, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882, 0.9882,\n",
       "          0.9882, 0.9882, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0863, 0.9882, 0.9569,\n",
       "          0.9098, 0.7686, 0.4941, 0.4941, 0.4941, 0.0824, 0.0824, 0.1922,\n",
       "          0.9882, 0.9882, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0275, 0.3294, 0.1922,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6000,\n",
       "          0.9882, 0.9882, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2784, 0.9765,\n",
       "          0.9882, 0.4745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1686, 0.8902, 0.9922,\n",
       "          0.8471, 0.0706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.9882, 0.9882,\n",
       "          0.9294, 0.1255, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0431, 0.3608, 0.8431, 0.9922, 0.9882, 0.9882,\n",
       "          0.9882, 0.9255, 0.2431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.2235, 0.8275, 0.9882, 0.9882, 0.9922, 0.6627, 0.3647,\n",
       "          0.8039, 0.9882, 0.9373, 0.2706, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.4196, 0.7804, 0.9882, 0.9882, 0.9882, 0.3373, 0.0431, 0.0000,\n",
       "          0.0431, 0.8549, 0.9922, 0.6588, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.9961, 0.9922, 0.9922, 0.7804, 0.0706, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.8314, 1.0000, 0.8549, 0.0471, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.8000, 0.6588, 0.4549, 0.0824, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.5137, 0.9922, 0.9882, 0.0824, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0353, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.4157, 0.9922, 0.9882, 0.0824, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.1373, 0.9922, 0.9059, 0.0627, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.3490, 0.9922, 0.6588, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.8314, 1.0000, 0.6588, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0863, 0.8824, 0.9922, 0.5176, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0784,\n",
       "          0.7922, 0.9882, 0.6314, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0863, 0.7922,\n",
       "          0.9882, 0.9882, 0.3137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3137, 0.8824, 0.9882,\n",
       "          0.9882, 0.2980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 768,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx, (imgs, _) in enumerate(train_loader):\n",
    "    break\n",
    "imgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "034848d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1596/1979287425.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0midx\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mreal_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mreal_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m#real_label = torch.reshape(real_label, (real_label.shape[0], 1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for idx, (imgs, labs) in enumerate(train_loader):\n",
    "        idx += 1\n",
    "\n",
    "        real_inputs = imgs.to(device)\n",
    "        real_label = labs.to(device)\n",
    "        #real_label = torch.reshape(real_label, (real_label.shape[0], 1))\n",
    "        real_outputs = D(real_inputs, real_label)\n",
    "\n",
    "\n",
    "        noise = torch.rand(batch_size, 100) \n",
    "        noise = noise.to(device)\n",
    "\n",
    "        \n",
    "        fake_labels = torch.LongTensor(np.random.randint(0, 10, batch_size)).cuda()\n",
    "        #fake_labels = torch.reshape(fake_labels, (fake_labels.shape[0], 1))\n",
    "        fake_inputs = G(noise, fake_labels)\n",
    "        fake_outputs = D(fake_inputs, fake_labels)\n",
    "\n",
    "        outputs = torch.cat((real_outputs, fake_outputs), 0)\n",
    "        targets = torch.cat((real_label, fake_labels), 0)\n",
    "        \n",
    "\n",
    "        D_loss = loss_D(outputs.float().cuda(), targets.float().cuda())\n",
    "        D_optimizer.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        for k in range(5):\n",
    "            # Training the generator\n",
    "            # For generator, goal is to make the discriminator believe everything is 1\n",
    "            noise = torch.rand(batch_size, 100)\n",
    "            noise = noise.to(device)\n",
    "            \n",
    "            \n",
    "            fake_labels = torch.LongTensor(np.random.randint(0, 10, batch_size)).cuda()\n",
    "            #fake_labels = torch.reshape(fake_labels, (fake_labels.shape[0], 1))\n",
    "            \n",
    "            fake_inputs = G(noise, fake_labels)\n",
    "            fake_outputs = D(fake_inputs, fake_labels)\n",
    "            fake_targets = torch.LongTensor(np.random.randint(0, 10, batch_size)).cuda()\n",
    "            G_loss = loss_G(fake_outputs.float(), fake_targets.float())\n",
    "            G_optimizer.zero_grad()\n",
    "            G_loss.backward()\n",
    "            G_optimizer.step()\n",
    "\n",
    "        if idx % 100 == 0 or idx == len(train_loader):\n",
    "            print('Epoch {} Iteration {}: discriminator_loss {:.3f} generator_loss {:.3f}'.format(epoch, idx, D_loss.item(), G_loss.item()))\n",
    "            fig, axs = plt.subplots(1, 4)\n",
    "            for i in range(4):\n",
    "                noise = torch.rand(1, 100)\n",
    "                noise = noise.to(device)\n",
    "                output = G(noise, torch.LongTensor([0]).cuda())\n",
    "                axs[i].imshow(output.permute(1, 2, 0).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6871d87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3318a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.LongTensor(np.random.randint(0, 10, 128)).cuda()\n",
    "print(test.shape)\n",
    "#test = torch.reshape(test, (test.shape[0], 1))\n",
    "embedding = nn.Embedding(10, 10).cuda()\n",
    "test = embedding(test)\n",
    "#test = torch.reshape(test, (test.shape[0], test.shape[2]))\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e742ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 5)\n",
    "for i in range(5):\n",
    "    noise = (torch.randn(real_inputs.shape[0], 128) - 0.5) / 0.5\n",
    "    noise = noise.to(device) \n",
    "    output = G(noise)\n",
    "    axs[i].imshow(output[3].permute(1, 2, 0).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "914ac215",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_INTERNAL_ERROR",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9176/759728191.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mD_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mD_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mD_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mD_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 396\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for idx, (imgs, _) in enumerate(train_loader):\n",
    "        idx += 1\n",
    "        \n",
    "\n",
    "        # Training the discriminator\n",
    "        # Real inputs are actual images of the MNIST dataset\n",
    "        # Fake inputs are from the generator\n",
    "        # Real inputs should be classified as 1 and fake as 0\n",
    "        real_inputs = imgs.cuda()\n",
    "        real_outputs = D(real_inputs)\n",
    "        real_label = torch.ones(real_inputs.shape[0], 1).to(device)\n",
    "\n",
    "        noise = (torch.rand(real_inputs.shape[0], 128) - 0.5) / 0.5\n",
    "        noise = noise.to(device)\n",
    "        fake_inputs = G(noise)\n",
    "        fake_outputs = D(fake_inputs)\n",
    "        fake_label = torch.zeros(fake_inputs.shape[0], 1).to(device)\n",
    "\n",
    "        outputs = torch.cat((real_outputs, fake_outputs), 0)\n",
    "        targets = torch.cat((real_label, fake_label), 0)\n",
    "\n",
    "        D_loss = loss_D(outputs.cuda(), targets.cuda())\n",
    "        D_optimizer.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        for k in range(5):\n",
    "            # Training the generator\n",
    "            # For generator, goal is to make the discriminator believe everything is 1\n",
    "            noise = (torch.rand(real_inputs.shape[0], 128)-0.5)/0.5\n",
    "            noise = noise.to(device)\n",
    "\n",
    "            fake_inputs = G(noise)\n",
    "            fake_outputs = D(fake_inputs)\n",
    "            fake_targets = torch.ones([fake_inputs.shape[0], 1]).to(device)\n",
    "            G_loss = loss_G(fake_outputs, fake_targets)\n",
    "            G_optimizer.zero_grad()\n",
    "            G_loss.backward()\n",
    "            G_optimizer.step()\n",
    "\n",
    "        if idx % 100 == 0 or idx == len(train_loader):\n",
    "            print('Epoch {} Iteration {}: discriminator_loss {:.3f} generator_loss {:.3f}'.format(epoch, idx, D_loss.item(), G_loss.item()))\n",
    "            fig, axs = plt.subplots(1, 4)\n",
    "            for i in range(4):\n",
    "                noise = (torch.randn(real_inputs.shape[0], 128) - 0.5) / 0.5\n",
    "                noise = noise.to(device) \n",
    "                output = G(noise)\n",
    "                axs[i].imshow(output[0].permute(1, 2, 0).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a498e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
